{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the training dataset, supplied by Kaggle <https://www.kaggle.com/c/santander-product-recommendation/data>\n",
    "\n",
    "train_ver2 = pd.read_csv(\"../../capstone_assets/train_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns never used in the study to reduce memory footprint. Identified by earlier EDA.\n",
    "\n",
    "for col in ['conyuemp','tipodom','indrel','ult_fec_cli_1t','indfall']:\n",
    "    train_ver2.drop(col,inplace=True,axis=1)\n",
    "\n",
    "# Data cleaning, filling missing values to reduce unnecessary blank or false dummy variables.    \n",
    "    \n",
    "train_ver2['ind_empleado'].fillna('N',inplace=True)\n",
    "train_ver2['pais_residencia'].fillna('ES',inplace=True)\n",
    "train_ver2['tiprel_1mes'].fillna('I',inplace=True)\n",
    "train_ver2['nomprov'].fillna('DESCONOCIDO',inplace=True)\n",
    "train_ver2['segmento'].fillna('02 - PARTICULARES',inplace=True)\n",
    "\n",
    "train_ver2['ind_empleado'].replace('S','P',inplace=True)\n",
    "train_ver2['age'].replace(to_replace=' NA',value=np.NaN,inplace=True)\n",
    "train_ver2.fecha_alta = train_ver2.fecha_alta.replace(to_replace=-9223372036854775808,value=np.NaN)\n",
    "train_ver2['antiguedad'].replace(to_replace=[' NA','     NA'],value=np.NaN,inplace=True)\n",
    "train_ver2.loc[train_ver2['antiguedad'] <0 , 'antiguedad'] = 0\n",
    "\n",
    "for col in ['fecha_dato','fecha_alta']:\n",
    "    train_ver2[col] = pd.to_datetime(train_ver2[col],yearfirst=True)\n",
    "\n",
    "# Convert columns from Objects, to move optimised data types. Reduces memory footprint, particularly important \n",
    "# given the scale of the data\n",
    "    \n",
    "for col in ['ncodpers', 'age', 'fecha_alta', 'ind_nuevo', 'antiguedad','indrel_1mes']:\n",
    "    train_ver2[col] = pd.to_numeric(train_ver2[col],downcast='unsigned',errors='ignore')\n",
    "for col in ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'canal_entrada','nomprov','ind_actividad_cliente','segmento']:\n",
    "    train_ver2[col] = train_ver2[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the product columns;\n",
    "# * filling blanks (missing products are products customers do not have)\n",
    "# * optimising the astype\n",
    "# * and capturing their names (for operations later)\n",
    "\n",
    "product_columns = []\n",
    "for column in train_ver2.columns:\n",
    "    if 'ind_' in column and '_ult1' in column:\n",
    "        train_ver2[column].fillna(0,inplace=True)\n",
    "        train_ver2[column] = train_ver2[column].astype('category')\n",
    "        product_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the columns used in modelling, per category.\n",
    "# Broken out so that they can be swapped in and out, and re-ran\n",
    "\n",
    "id_columns = ['ncodpers']\n",
    "date_columns = ['fecha_dato']\n",
    "numerical_columns = ['fecha_alta','antiguedad','renta',]\n",
    "categorical_columns = ['ind_empleado','sexo','indrel_1mes','indresi','segmento','ind_nuevo','ind_actividad_cliente','nomprov']\n",
    "\n",
    "# These categorical columns are not included in the model, as the weight/dimensionality they add\n",
    "# isn't worth the modelling value after initial experimentation. But, others may wish to add them back\n",
    "\n",
    "reduced_categorical_columns = ['canal_entrada','pais_residencia','tiprel_1mes','indext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The modelling is based off three dataframes\n",
    "# *w, meaning wide, which is the dataframe of just the selected features, with categories dummied\n",
    "train_ver2w = pd.concat([train_ver2[id_columns],train_ver2[date_columns],train_ver2[numerical_columns],pd.get_dummies(train_ver2[categorical_columns],dummy_na=True),train_ver2[product_columns]],axis=1)\n",
    "\n",
    "# *b, meaning before, which is the training data\n",
    "train_ver2b = train_ver2w[train_ver2w['fecha_dato'].isin(['28-04-2016'])]\n",
    "train_ver2b.drop(['fecha_dato'], axis=1, inplace = True)\n",
    "\n",
    "# *a, meaning after, which is the test data\n",
    "train_ver2a = train_ver2w[train_ver2w['fecha_dato'].isin(['28-05-2016'])]\n",
    "train_ver2a.drop(['fecha_dato'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test data from kaggle.\n",
    "# The test data customers in June, aligns with the *a data from the training data in May - exactly the same customers, IDs etc\n",
    "\n",
    "test_ver2 = pd.read_csv(\"../../capstone_assets/test_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the data expands.\n",
    "# For each data set, (a + b), for each prior month, the product set of the customer, is appended.\n",
    "# So each dataset has a full product history per customer\n",
    "# This product history, although heavy in data terms, has the predictive value of boosting the MAP@7 score\n",
    "# from ~2.3 to ~2.6, vs just using one month to another to predict.\n",
    "\n",
    "dates = train_ver2.fecha_dato.value_counts().index.values\n",
    "\n",
    "for date in dates[2:]:\n",
    "    dftr_prev = train_ver2[train_ver2['fecha_dato'].isin([date])][product_columns + ['ncodpers']]\n",
    "    train_ver2b = pd.merge(train_ver2b,dftr_prev, how='left', on=['ncodpers'], suffixes=('', '_'+str(date)[0:10]))\n",
    "    \n",
    "# The 'after' set does not include the very first month of historical data, so the number of months, and dimensions\n",
    "# match across the two data sets.\n",
    "\n",
    "for date in dates[1:16]:\n",
    "    dftr_prev = train_ver2[train_ver2['fecha_dato'].isin([date])][product_columns + ['ncodpers']]\n",
    "    train_ver2a = pd.merge(train_ver2a,dftr_prev, how='left', on=['ncodpers'], suffixes=('', '_'+str(date)[0:10]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training target, which is the may product portfolio per customer\n",
    "\n",
    "y_train = pd.merge(train_ver2b[['ncodpers']],train_ver2[train_ver2['fecha_dato'].isin(['25-05-2016'])][['ind_ahor_fin_ult1'] + ['ncodpers']], how='left', on=['ncodpers'])\n",
    "y_train.drop(['ncodpers'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simplify operations, I constrain the training data set to the customers who exist in the test data.\n",
    "# this drops thousands of customers, and potentially reduces predictive accuracy, but greatly increases calculation simplicity\n",
    "\n",
    "two_key_months = pd.Series(train_ver2[train_ver2['fecha_dato'].isin(['28-05-2016','28-04-2016'])]['ncodpers'].value_counts())\n",
    "two_key_months = two_key_months[two_key_months == 2]\n",
    "train_ver2b = train_ver2b[train_ver2b['ncodpers'].isin(two_key_months.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With the data cleaned, and training and test data defined, now the modelling.\n",
    "# The objective is relatively simple, but computationally intensive.\n",
    "# * We build a productive model for each product\n",
    "# * For the training, the target is each product, per customer, for May 2016. The predictor variables are the \n",
    "#   April demographics, and the monthly product history of the customer back 15 months.\n",
    "# * For the test, the target is each product, per customer, but June 2016. The predictors are the May demographics,\n",
    "#   and the product history also going back 15 months.\n",
    "\n",
    "# Then, using the model per product, per customer we rule out the products they already have, and predict\n",
    "# the products for June with the highest proba, and export it using Kaggle's required format, for a top 1000 score\n",
    "\n",
    "model_preds = {}\n",
    "id_preds = defaultdict(list)\n",
    "\n",
    "model_preds_train = {}\n",
    "id_preds_train = defaultdict(list)\n",
    "\n",
    "ids = train_ver2b['ncodpers'].values\n",
    "testids = train_ver2a['ncodpers'].values\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for c in product_columns:\n",
    "    print(c)\n",
    "    print datetime.now()    \n",
    "    print(counter)\n",
    "    counter += 1\n",
    "\n",
    "    y_train = pd.merge(train_ver2b[['ncodpers']],train_ver2[train_ver2['fecha_dato'].isin(['28-05-2016'])][[c] + ['ncodpers']], how='left', on=['ncodpers'])\n",
    "    y_train.drop(['ncodpers'],1,inplace=True)\n",
    "    x_train = train_ver2b.drop(['ncodpers'],1,inplace=False)\n",
    "    x_test = train_ver2a.drop(['ncodpers'],1,inplace=False)\n",
    "\n",
    "    \n",
    "    xgbcla = XGBClassifier(objective=\"binary:logistic\", n_estimators=200,max_depth=4,min_child_weight=3, learning_rate=0.05)  \n",
    "    xgbcla.fit(x_train.as_matrix(), y_train.values)  \n",
    "        \n",
    "    p_test = xgbcla.predict_proba(x_test.as_matrix())[:,1]\n",
    "        \n",
    "    model_preds[c] = p_test\n",
    "    for id, p in zip(testids, p_test):\n",
    "        id_preds[id].append(p)\n",
    "    \n",
    "already_active = {}\n",
    "\n",
    "for row in train_ver2a[['ncodpers']+product_columns].values:\n",
    "    row = list(row)\n",
    "    id = row.pop(0)\n",
    "    active = [c[0] for c in zip(product_columns, row) if c[1] > 0]\n",
    "    already_active[id] = active\n",
    "\n",
    "test_preds = {}\n",
    "for id, p in id_preds.items():\n",
    "    preds = [i[0] for i in sorted([i for i in zip(product_columns, p) if i[0] not in already_active[id]], key=lambda i:i [1], reverse=True)[:7]]\n",
    "    test_preds[id] = preds\n",
    "\n",
    "sample = pd.read_csv('../../capstone_assets/sample_submission.csv')\n",
    "\n",
    "test_preds_prods = []\n",
    "for row in sample.values:\n",
    "    id = row[0]\n",
    "    p = test_preds[id]\n",
    "    test_preds_prods.append(' '.join(p))\n",
    "\n",
    "sample['added_products'] = test_preds_prods\n",
    "sample.to_csv('2017-06-19_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
